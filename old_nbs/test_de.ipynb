{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# import stuff for creating class\n",
    "\n",
    "from abc import ABC, abstractmethod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "class TfIdfBase(ABC):\n",
    "    def __init__(self, corpus, docids, save_folder):\n",
    "        self.corpus = corpus\n",
    "        self.docids = docids\n",
    "        self.save_folder = save_folder\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_idf(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_tf(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def preprocess(self):\n",
    "        pass\n",
    "\n",
    "    def compute_tf_idf(self, do_normalize=True):\n",
    "        self.tf_idf_matrix = self.tf_matrix * self.idf_matrix\n",
    "        # row-wise L2 norm\n",
    "        if do_normalize:\n",
    "            self.tf_idf_matrix = normalize(self.tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stopwordsiso as stopwords\n",
    "import re\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "import os\n",
    "import collections\n",
    "\n",
    "class TfIdfGerman(TfIdfBase):\n",
    "    def __init__(self, corpus, docids, save_folder, overwrite=False):\n",
    "        super().__init__(corpus, docids, save_folder)\n",
    "        self.stopwords = stopwords.stopwords(\"de\")\n",
    "        self.overwrite = overwrite\n",
    "\n",
    "        os.makedirs(f'{self.save_folder}/de', exist_ok=True)\n",
    "\n",
    "    def preprocess(self):\n",
    "\n",
    "        if not self.overwrite and os.path.exists(f'{self.save_folder}/de/word_list.pkl'):\n",
    "            logger.info(\"Loading unique words and word list\")\n",
    "            with open(f'{self.save_folder}/de/word_list.pkl', 'rb') as f:\n",
    "                self.unique_words, self.word_list, self.word_to_index = pickle.load(f)\n",
    "\n",
    "            return  \n",
    "\n",
    "        logger.info(\"Preprocessing corpus\")\n",
    "        start = time.perf_counter()\n",
    "        # self.corpus = [re.sub(r'[^\\w\\s]', doc.lower()) for doc in tqdm(self.corpus)]\n",
    "        # self.corpus = [[word for word in doc.split() if word not in self.stopwords] for doc in tqdm(self.corpus)]\n",
    "\n",
    "        # combine the above 2 into 1\n",
    "        self.word_list = [[word for word in re.sub(r'[^\\w\\s]', '', doc.lower()).split()\\\n",
    "                        if word not in self.stopwords] for doc in tqdm(self.corpus)]\n",
    "\n",
    "\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        self.unique_words = set()\n",
    "        for doc in tqdm(self.word_list):\n",
    "            self.unique_words.update(doc)\n",
    "\n",
    "        self.unique_words = sorted(list(self.unique_words))\n",
    "        self.word_to_index = {word: i for i, word in enumerate(self.unique_words)}\n",
    "\n",
    "        logger.info(f\"Preprocessing took {end-start:.2f} seconds\")\n",
    "        logger.info(f\"Number of unique words: {len(self.unique_words)}\")\n",
    "\n",
    "        logger.info(f\"Saving unique words and word list to {self.save_folder}/de\")\n",
    "        with open(f'{self.save_folder}/de/word_list.pkl', 'wb') as f:\n",
    "            pickle.dump((self.unique_words, self.word_list, self.word_to_index), f)\n",
    "\n",
    "    def compute_tf(self):\n",
    "        # computing term frequency\n",
    "\n",
    "        # if not self.overwrite and os.path.exists(f'{self.save_folder}/en/tf_matrix.npy'):\n",
    "        #     logger.info(\"Loading tf matrix\")\n",
    "        #     self.tf_matrix = np.load(f'{self.save_folder}/en/tf_matrix.npy')\n",
    "        #     return self.tf_matrix\n",
    "\n",
    "        # logger.info(\"Computing term frequency\")\n",
    "        # start = time.perf_counter()\n",
    "\n",
    "        # self.tf_matrix = scipy.sparse.csr_matrix(np.zeros((len(self.docids), len(self.unique_words)), dtype=np.int32))\n",
    "\n",
    "        # for i, doc in enumerate(tqdm(self.word_list)):\n",
    "        #     for word in doc:\n",
    "        #         self.tf_matrix[i, self.word_to_index[word]] += 1\n",
    "\n",
    "        # end = time.perf_counter()\n",
    "        # logger.info(f\"Computing term frequency took {end-start:.2f} seconds\")\n",
    "\n",
    "        # logger.info(f\"Saving tf matrix to {self.save_folder}/en\")\n",
    "        # np.save(f'{self.save_folder}/en/tf_matrix.npy', self.tf_matrix)\n",
    "        # return self.tf_matrix\n",
    "        self._compute_sparse_tf()\n",
    "    \n",
    "\n",
    "    def _compute_sparse_tf(self):\n",
    "        if not self.overwrite and os.path.exists(f'{self.save_folder}/de/sparse_tf_matrix.npy'):\n",
    "            logger.info(\"Loading tf matrix\")\n",
    "            self.tf_matrix = np.load(f'{self.save_folder}/de/sparse_tf_matrix.npy', allow_pickle=True)\n",
    "            return self.tf_matrix\n",
    "        start = time.perf_counter()\n",
    "        row, col, data = [], [], []\n",
    "        for i, doc in enumerate(tqdm(self.word_list)):\n",
    "            word_count = collections.defaultdict(int)\n",
    "            for word in doc:\n",
    "                word_count[word] += 1\n",
    "\n",
    "\n",
    "            for word, count in word_count.items():\n",
    "                row.append(i)\n",
    "                col.append(self.word_to_index[word])\n",
    "                data.append(count)\n",
    "\n",
    "        self.tf_matrix = scipy.sparse.csr_matrix((data, (row, col)), shape=(len(self.docids), len(self.unique_words)), dtype=np.int32)\n",
    "        end = time.perf_counter()\n",
    "        logger.info(f\"Computing term frequency took {end-start:.2f} seconds\")\n",
    "\n",
    "        logger.info(f\"Saving tf matrix to {self.save_folder}/de\")\n",
    "        np.save(f'{self.save_folder}/de/sparse_tf_matrix.npy', self.tf_matrix)\n",
    "        # return self.tf_matrix\n",
    "\n",
    "\n",
    "    def _compute_sparse_idf(self):\n",
    "        if not self.overwrite and os.path.exists(f'{self.save_folder}/de/sparse_idf_matrix.npy'):\n",
    "            logger.info(\"Loading idf matrix\")\n",
    "            self.idf_matrix = np.load(f'{self.save_folder}/de/sparse_idf_matrix.npy', allow_pickle=True)\n",
    "            return self.idf_matrix\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        self.idf_matrix = (self.tf_matrix > 0).sum(axis=0)\n",
    "        self.idf_matrix = np.squeeze(np.array(self.idf_matrix))\n",
    "        self.idf_matrix = np.log((1 + len(self.docids)) / (1 + self.idf_matrix)) + 1\n",
    "        print(self.idf_matrix)\n",
    "        self.idf_matrix = scipy.sparse.diags(self.idf_matrix.astype(np.float32))\n",
    "        end = time.perf_counter()\n",
    "        logger.info(f\"Computing inverse document frequency took {end-start:.2f} seconds\")\n",
    "\n",
    "        logger.info(f\"Saving idf matrix to {self.save_folder}/de\")\n",
    "        np.save(f'{self.save_folder}/de/sparse_idf_matrix.npy', self.idf_matrix)\n",
    "        # return self.idf_matrix\n",
    "    \n",
    "    def compute_idf(self):\n",
    "        # return self._compute_sparse_idf()\n",
    "        self._compute_sparse_idf()\n",
    "\n",
    "\n",
    "    def _get_sparse_query_tfidf(self, queries, use_idf, do_normalize):\n",
    "        query_word_list = [[word for word in re.sub(r'[^\\w\\s]', '', query.lower()).split()\\\n",
    "                        if word not in self.stopwords] for query in queries]\n",
    "\n",
    "        # query_vectors = np.zeros((len(queries), len(self.unique_words)))\n",
    "        # for i in range(len(queries)):\n",
    "        #     for word in query_word_list[i]:\n",
    "        #         if word in self.word_to_index:\n",
    "        #             query_vectors[i][self.word_to_index[word]] = \n",
    "\n",
    "        row, col, data = [], [], []\n",
    "\n",
    "        for i, doc in enumerate(query_word_list):\n",
    "            word_count = collections.defaultdict(int)\n",
    "            for word in doc:\n",
    "                if word in self.word_to_index:\n",
    "                    word_count[word] += 1\n",
    "\n",
    "            for word, count in word_count.items():\n",
    "                row.append(i)\n",
    "                col.append(self.word_to_index[word])\n",
    "                data.append(count)\n",
    "\n",
    "        query_tf_idf_matrix = scipy.sparse.csr_matrix((data, (row, col)), shape=(len(queries), len(self.unique_words)), dtype=np.int32)\n",
    "\n",
    "        if use_idf:\n",
    "            query_tf_idf_matrix = np.array(query_tf_idf_matrix)\n",
    "            query_tf_idf_matrix = query_tf_idf_matrix * self.idf_matrix\n",
    "        \n",
    "        if do_normalize:\n",
    "            query_tf_idf_matrix = normalize(query_tf_idf_matrix)\n",
    "\n",
    "        return query_tf_idf_matrix\n",
    "\n",
    "\n",
    "\n",
    "    def process_queries(self, queries, topk=5, use_idf=False, do_normalize=True):\n",
    "\n",
    "        assert len(queries) == 1\n",
    "        query_tf_idf_matrix = self._get_sparse_query_tfidf(queries, use_idf, do_normalize)\n",
    "\n",
    "        cosine_similarities = self.tf_idf_matrix.dot(query_tf_idf_matrix.T).toarray().flatten()\n",
    "        top_k_indices = np.argsort(cosine_similarities)[::-1][:topk]  # Get top 10 indices by score\n",
    "\n",
    "        return [self.docids[i] for i in top_k_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english_corpus = [\n",
    "#     \"this is the first document psosp\",\n",
    "#     \"this document is the second document ssdfa\",\n",
    "#     \"and this is the third one\",\n",
    "#     \"is this the first document\"\n",
    "# ]\n",
    "# english_docids = list(range(len(english_corpus)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('corpus.json') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_corpus = [doc['text'] for doc in corpus if doc['lang'] == 'de']\n",
    "german_docids = [doc['docid'] for doc in corpus if doc['lang'] == 'de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFV = TfIdfGerman(german_corpus, german_docids, 'tfidf', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-18 16:17:49.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mPreprocessing corpus\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49841fa8c484f4ab65e17fb995cbd23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10992 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a0e869efef45a5b553d4717fd5ae00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10992 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-18 16:18:26.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mPreprocessing took 16.42 seconds\u001b[0m\n",
      "\u001b[32m2024-10-18 16:18:26.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mNumber of unique words: 2454116\u001b[0m\n",
      "\u001b[32m2024-10-18 16:18:26.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mSaving unique words and word list to tfidf/de\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "TFV.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078064ea49064fe0b25379459bbf36fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10992 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-18 16:19:09.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_compute_sparse_tf\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mComputing term frequency took 18.02 seconds\u001b[0m\n",
      "\u001b[32m2024-10-18 16:19:09.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_compute_sparse_tf\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mSaving tf matrix to tfidf/de\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "TFV.compute_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-18 16:19:15.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_compute_sparse_idf\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mComputing inverse document frequency took 0.19 seconds\u001b[0m\n",
      "\u001b[32m2024-10-18 16:19:15.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_compute_sparse_idf\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mSaving idf matrix to tfidf/de\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.56914367 3.89155503 3.48172786 ... 9.61186681 9.61186681 9.61186681]\n"
     ]
    }
   ],
   "source": [
    "TFV.compute_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFV.compute_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dev_set = pd.read_csv('./dev.csv')\n",
    "german_dev_set = dev_set[dev_set['lang'] == 'de']\n",
    "german_dev_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_dev_set_queries = german_dev_set['query'].tolist()\n",
    "german_dev_set_positive_docs = german_dev_set['positive_docs'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_10(positive_docs, top_10_ids):\n",
    "    recall = []\n",
    "    for positive_doc, top_10_id in zip(positive_docs, top_10_ids):\n",
    "        recall.append(positive_doc in top_10_id)\n",
    "\n",
    "    print(np.mean(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89872cdacaaa449d82464416597ed1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m top_10_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dev_query \u001b[38;5;129;01min\u001b[39;00m tqdm(german_dev_set_queries):\n\u001b[0;32m----> 3\u001b[0m     top_10_ids\u001b[38;5;241m.\u001b[39mappend(\u001b[43mTFV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdev_query\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[3], line 170\u001b[0m, in \u001b[0;36mTfIdfGerman.process_queries\u001b[0;34m(self, queries, topk, use_idf, do_normalize)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_queries\u001b[39m(\u001b[38;5;28mself\u001b[39m, queries, topk\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, do_normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(queries) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 170\u001b[0m     query_tf_idf_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_sparse_query_tfidf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_idf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_normalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     cosine_similarities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_idf_matrix\u001b[38;5;241m.\u001b[39mdot(query_tf_idf_matrix\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mtoarray()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    173\u001b[0m     top_k_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(cosine_similarities)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:topk]  \u001b[38;5;66;03m# Get top 10 indices by score\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 161\u001b[0m, in \u001b[0;36mTfIdfGerman._get_sparse_query_tfidf\u001b[0;34m(self, queries, use_idf, do_normalize)\u001b[0m\n\u001b[1;32m    158\u001b[0m     query_tf_idf_matrix \u001b[38;5;241m=\u001b[39m query_tf_idf_matrix \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midf_matrix\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_normalize:\n\u001b[0;32m--> 161\u001b[0m     query_tf_idf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_tf_idf_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_tf_idf_matrix\n",
      "File \u001b[0;32m~/anaconda3/envs/ada/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:1817\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1815\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a supported axis\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m axis)\n\u001b[0;32m-> 1817\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1819\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthe normalize function\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1823\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1825\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/anaconda3/envs/ada/lib/python3.11/site-packages/sklearn/utils/validation.py:845\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(array):\n\u001b[1;32m    844\u001b[0m     _ensure_no_complex_data(array)\n\u001b[0;32m--> 845\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_sparse_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;66;03m# to an error. This is needed because specifying a non complex\u001b[39;00m\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;66;03m# dtype to the function converts complex to real dtype,\u001b[39;00m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# thereby passing the test made in the lines following the scope\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# of warnings context manager.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n",
      "File \u001b[0;32m~/anaconda3/envs/ada/lib/python3.11/site-packages/sklearn/utils/validation.py:537\u001b[0m, in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;66;03m# ensure correct sparse format\u001b[39;00m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spmatrix\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m accept_sparse:\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;66;03m# create new with correct sparse\u001b[39;00m\n\u001b[0;32m--> 537\u001b[0m         spmatrix \u001b[38;5;241m=\u001b[39m \u001b[43mspmatrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m         changed_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m accept_sparse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;66;03m# any other type\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ada/lib/python3.11/site-packages/scipy/sparse/_base.py:435\u001b[0m, in \u001b[0;36m_spbase.asformat\u001b[0;34m(self, format, copy)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# Forward the copy kwarg, if it's accepted.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_method()\n",
      "File \u001b[0;32m~/anaconda3/envs/ada/lib/python3.11/site-packages/scipy/sparse/_base.py:1002\u001b[0m, in \u001b[0;36m_spbase.tocsr\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtocsr\u001b[39m(\u001b[38;5;28mself\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    997\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert this array/matrix to Compressed Sparse Row format.\u001b[39;00m\n\u001b[1;32m    998\u001b[0m \n\u001b[1;32m    999\u001b[0m \u001b[38;5;124;03m    With copy=False, the data/indices may be shared between this array/matrix and\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;124;03m    the resultant csr_array/matrix.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1002\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtocoo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtocsr(copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ada/lib/python3.11/site-packages/scipy/sparse/_dia.py:342\u001b[0m, in \u001b[0;36m_dia_base.tocoo\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    340\u001b[0m mask \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m (row \u001b[38;5;241m<\u001b[39m num_rows)\n\u001b[1;32m    341\u001b[0m mask \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m (offset_inds \u001b[38;5;241m<\u001b[39m num_cols)\n\u001b[0;32m--> 342\u001b[0m mask \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m)\n\u001b[1;32m    343\u001b[0m row \u001b[38;5;241m=\u001b[39m row[mask]\n\u001b[1;32m    344\u001b[0m col \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(offset_inds, num_offsets)[mask\u001b[38;5;241m.\u001b[39mravel()]\n",
      "File \u001b[0;32m~/anaconda3/envs/ada/lib/python3.11/site-packages/scipy/sparse/_base.py:396\u001b[0m, in \u001b[0;36m_spbase.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnnz \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 396\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of an array with more than one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    397\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melement is ambiguous. Use a.any() or a.all().\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "top_10_ids = []\n",
    "for dev_query in tqdm(german_dev_set_queries):\n",
    "    top_10_ids.append(TFV.process_queries([dev_query], 10, True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63\n"
     ]
    }
   ],
   "source": [
    "recall_at_10(german_dev_set_positive_docs, top_10_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a0c227bca143bcadb9df2d491bfb94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_10_ids = []\n",
    "for dev_query in tqdm(german_dev_set_queries):\n",
    "    top_10_ids.append(TFV.process_queries([dev_query], 10, False, True))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15b992d29cd4b9ca4163dd937d3508b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_10_ids = []\n",
    "for dev_query in tqdm(german_dev_set_queries):\n",
    "    top_10_ids.append(TFV.process_queries([dev_query], 10, True, False))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63\n"
     ]
    }
   ],
   "source": [
    "recall_at_10(german_dev_set_positive_docs, top_10_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd trial\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_tfidf_matrix = svd.fit_transform(TFV.tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
