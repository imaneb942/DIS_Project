{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# import stuff for creating class\n",
    "\n",
    "from abc import ABC, abstractmethod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "class TfIdfBase(ABC):\n",
    "    def __init__(self, corpus, docids, save_folder):\n",
    "        self.corpus = corpus\n",
    "        self.docids = docids\n",
    "        self.save_folder = save_folder\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_idf(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_tf(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def preprocess(self):\n",
    "        pass\n",
    "\n",
    "    def compute_tf_idf(self, do_normalize=True):\n",
    "        self.tf_idf_matrix = self.tf_matrix * self.idf_matrix\n",
    "        # row-wise L2 norm\n",
    "        if do_normalize:\n",
    "            self.tf_idf_matrix = normalize(self.tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stopwordsiso as stopwords\n",
    "import re\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "import os\n",
    "import collections\n",
    "\n",
    "class TfIdfFrench(TfIdfBase):\n",
    "    def __init__(self, corpus, docids, save_folder, overwrite=False):\n",
    "        super().__init__(corpus, docids, save_folder)\n",
    "        self.stopwords = stopwords.stopwords(\"fr\")\n",
    "        self.overwrite = overwrite\n",
    "\n",
    "        os.makedirs(f'{self.save_folder}/fr', exist_ok=True)\n",
    "\n",
    "    def preprocess(self):\n",
    "\n",
    "        if not self.overwrite and os.path.exists(f'{self.save_folder}/fr/word_list.pkl'):\n",
    "            logger.info(\"Loading unique words and word list\")\n",
    "            with open(f'{self.save_folder}/fr/word_list.pkl', 'rb') as f:\n",
    "                self.unique_words, self.word_list, self.word_to_index = pickle.load(f)\n",
    "\n",
    "            return  \n",
    "\n",
    "        logger.info(\"Preprocessing corpus\")\n",
    "        start = time.perf_counter()\n",
    "        # self.corpus = [re.sub(r'[^\\w\\s]', doc.lower()) for doc in tqdm(self.corpus)]\n",
    "        # self.corpus = [[word for word in doc.split() if word not in self.stopwords] for doc in tqdm(self.corpus)]\n",
    "\n",
    "        # combine the above 2 into 1\n",
    "        self.word_list = [[word for word in re.sub(r'[^\\w\\s]', '', doc.lower()).split()\\\n",
    "                        if word not in self.stopwords] for doc in tqdm(self.corpus)]\n",
    "\n",
    "\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        self.unique_words = set()\n",
    "        for doc in tqdm(self.word_list):\n",
    "            self.unique_words.update(doc)\n",
    "\n",
    "        self.unique_words = sorted(list(self.unique_words))\n",
    "        self.word_to_index = {word: i for i, word in enumerate(self.unique_words)}\n",
    "\n",
    "        logger.info(f\"Preprocessing took {end-start:.2f} seconds\")\n",
    "        logger.info(f\"Number of unique words: {len(self.unique_words)}\")\n",
    "\n",
    "        logger.info(f\"Saving unique words and word list to {self.save_folder}/fr\")\n",
    "        with open(f'{self.save_folder}/fr/word_list.pkl', 'wb') as f:\n",
    "            pickle.dump((self.unique_words, self.word_list, self.word_to_index), f)\n",
    "\n",
    "    def compute_tf(self):\n",
    "        # computing term frequency\n",
    "\n",
    "        # if not self.overwrite and os.path.exists(f'{self.save_folder}/en/tf_matrix.npy'):\n",
    "        #     logger.info(\"Loading tf matrix\")\n",
    "        #     self.tf_matrix = np.load(f'{self.save_folder}/en/tf_matrix.npy')\n",
    "        #     return self.tf_matrix\n",
    "\n",
    "        # logger.info(\"Computing term frequency\")\n",
    "        # start = time.perf_counter()\n",
    "\n",
    "        # self.tf_matrix = scipy.sparse.csr_matrix(np.zeros((len(self.docids), len(self.unique_words)), dtype=np.int32))\n",
    "\n",
    "        # for i, doc in enumerate(tqdm(self.word_list)):\n",
    "        #     for word in doc:\n",
    "        #         self.tf_matrix[i, self.word_to_index[word]] += 1\n",
    "\n",
    "        # end = time.perf_counter()\n",
    "        # logger.info(f\"Computing term frequency took {end-start:.2f} seconds\")\n",
    "\n",
    "        # logger.info(f\"Saving tf matrix to {self.save_folder}/en\")\n",
    "        # np.save(f'{self.save_folder}/en/tf_matrix.npy', self.tf_matrix)\n",
    "        # return self.tf_matrix\n",
    "        self._compute_sparse_tf()\n",
    "    \n",
    "\n",
    "    def _compute_sparse_tf(self):\n",
    "        if not self.overwrite and os.path.exists(f'{self.save_folder}/fr/sparse_tf_matrix.npy'):\n",
    "            logger.info(\"Loading tf matrix\")\n",
    "            self.tf_matrix = np.load(f'{self.save_folder}/fr/sparse_tf_matrix.npy', allow_pickle=True)\n",
    "            return self.tf_matrix\n",
    "        start = time.perf_counter()\n",
    "        row, col, data = [], [], []\n",
    "        for i, doc in enumerate(tqdm(self.word_list)):\n",
    "            word_count = collections.defaultdict(int)\n",
    "            for word in doc:\n",
    "                word_count[word] += 1\n",
    "\n",
    "\n",
    "            for word, count in word_count.items():\n",
    "                row.append(i)\n",
    "                col.append(self.word_to_index[word])\n",
    "                data.append(count)\n",
    "\n",
    "        self.tf_matrix = scipy.sparse.csr_matrix((data, (row, col)), shape=(len(self.docids), len(self.unique_words)), dtype=np.int32)\n",
    "        end = time.perf_counter()\n",
    "        logger.info(f\"Computing term frequency took {end-start:.2f} seconds\")\n",
    "\n",
    "        logger.info(f\"Saving tf matrix to {self.save_folder}/fr\")\n",
    "        np.save(f'{self.save_folder}/fr/sparse_tf_matrix.npy', self.tf_matrix)\n",
    "        # return self.tf_matrix\n",
    "\n",
    "\n",
    "    def _compute_sparse_idf(self):\n",
    "        if not self.overwrite and os.path.exists(f'{self.save_folder}/fr/sparse_idf_matrix.npy'):\n",
    "            logger.info(\"Loading idf matrix\")\n",
    "            self.idf_matrix = np.load(f'{self.save_folder}/fr/sparse_idf_matrix.npy', allow_pickle=True)\n",
    "            return self.idf_matrix\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        self.idf_matrix = (self.tf_matrix > 0).sum(axis=0)\n",
    "        self.idf_matrix = np.squeeze(np.array(self.idf_matrix))\n",
    "        self.idf_matrix = np.log((1 + len(self.docids)) / (1 + self.idf_matrix)) + 1\n",
    "        print(self.idf_matrix)\n",
    "        self.idf_matrix = scipy.sparse.diags(self.idf_matrix.astype(np.float32))\n",
    "        end = time.perf_counter()\n",
    "        logger.info(f\"Computing inverse document frequency took {end-start:.2f} seconds\")\n",
    "\n",
    "        logger.info(f\"Saving idf matrix to {self.save_folder}/fr\")\n",
    "        np.save(f'{self.save_folder}/fr/sparse_idf_matrix.npy', self.idf_matrix)\n",
    "        # return self.idf_matrix\n",
    "    \n",
    "    def compute_idf(self):\n",
    "        # return self._compute_sparse_idf()\n",
    "        self._compute_sparse_idf()\n",
    "\n",
    "\n",
    "    def _get_sparse_query_tfidf(self, queries, use_idf, do_normalize):\n",
    "        query_word_list = [[word for word in re.sub(r'[^\\w\\s]', '', query.lower()).split()\\\n",
    "                        if word not in self.stopwords] for query in queries]\n",
    "\n",
    "        # query_vectors = np.zeros((len(queries), len(self.unique_words)))\n",
    "        # for i in range(len(queries)):\n",
    "        #     for word in query_word_list[i]:\n",
    "        #         if word in self.word_to_index:\n",
    "        #             query_vectors[i][self.word_to_index[word]] = \n",
    "\n",
    "        row, col, data = [], [], []\n",
    "\n",
    "        for i, doc in enumerate(query_word_list):\n",
    "            word_count = collections.defaultdict(int)\n",
    "            for word in doc:\n",
    "                if word in self.word_to_index:\n",
    "                    word_count[word] += 1\n",
    "\n",
    "            for word, count in word_count.items():\n",
    "                row.append(i)\n",
    "                col.append(self.word_to_index[word])\n",
    "                data.append(count)\n",
    "\n",
    "        query_tf_idf_matrix = scipy.sparse.csr_matrix((data, (row, col)), shape=(len(queries), len(self.unique_words)), dtype=np.int32)\n",
    "\n",
    "        if use_idf:\n",
    "            query_tf_idf_matrix = np.array(query_tf_idf_matrix)\n",
    "            query_tf_idf_matrix = query_tf_idf_matrix * self.idf_matrix\n",
    "        \n",
    "        if do_normalize:\n",
    "            query_tf_idf_matrix = normalize(query_tf_idf_matrix)\n",
    "\n",
    "        return query_tf_idf_matrix\n",
    "\n",
    "\n",
    "\n",
    "    def process_queries(self, queries, topk=5, use_idf=False, do_normalize=True):\n",
    "\n",
    "        assert len(queries) == 1\n",
    "        query_tf_idf_matrix = self._get_sparse_query_tfidf(queries, use_idf, do_normalize)\n",
    "\n",
    "        cosine_similarities = self.tf_idf_matrix.dot(query_tf_idf_matrix.T).toarray().flatten()\n",
    "        top_k_indices = np.argsort(cosine_similarities)[::-1][:topk]  # Get top 10 indices by score\n",
    "\n",
    "        return [self.docids[i] for i in top_k_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english_corpus = [\n",
    "#     \"this is the first document psosp\",\n",
    "#     \"this document is the second document ssdfa\",\n",
    "#     \"and this is the third one\",\n",
    "#     \"is this the first document\"\n",
    "# ]\n",
    "# english_docids = list(range(len(english_corpus)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./corpus.json/corpus.json') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_corpus = [doc['text'] for doc in corpus if doc['lang'] == 'fr']\n",
    "english_docids = [doc['docid'] for doc in corpus if doc['lang'] == 'fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFV = TfIdfFrench(english_corpus, english_docids, 'tfidf', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-14 17:01:12.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mLoading unique words and word list\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "TFV.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-14 17:01:36.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_compute_sparse_tf\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mLoading tf matrix\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "TFV.compute_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-14 17:01:39.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_compute_sparse_idf\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mLoading idf matrix\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "TFV.compute_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFV.compute_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dev_set = pd.read_csv('./dev.csv')\n",
    "french_dev_set = dev_set[dev_set['lang'] == 'fr']\n",
    "french_dev_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_dev_set_queries = french_dev_set['query'].tolist()\n",
    "french_dev_set_positive_docs = french_dev_set['positive_docs'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_10(positive_docs, top_10_ids):\n",
    "    recall = []\n",
    "    for positive_doc, top_10_id in zip(positive_docs, top_10_ids):\n",
    "        recall.append(positive_doc in top_10_id)\n",
    "\n",
    "    print(np.mean(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194f1f60457b430e9c75974f9d536482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_10_ids = []\n",
    "for dev_query in tqdm(french_dev_set_queries):\n",
    "    top_10_ids.append(TFV.process_queries([dev_query], 10, True, True))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815\n"
     ]
    }
   ],
   "source": [
    "recall_at_10(french_dev_set_positive_docs, top_10_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19228c53ebd64b11b1e1995cb629507c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_10_ids = []\n",
    "for dev_query in tqdm(french_dev_set_queries):\n",
    "    top_10_ids.append(TFV.process_queries([dev_query], 10, False, True))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48348d79b5d24059a90e9d1ce0048fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_10_ids = []\n",
    "for dev_query in tqdm(french_dev_set_queries):\n",
    "    top_10_ids.append(TFV.process_queries([dev_query], 10, True, False))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815\n"
     ]
    }
   ],
   "source": [
    "recall_at_10(french_dev_set_positive_docs, top_10_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd trial\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m reduced_tfidf_matrix \u001b[38;5;241m=\u001b[39m svd\u001b[38;5;241m.\u001b[39mfit_transform(TFV\u001b[38;5;241m.\u001b[39mtf_idf_matrix)\n",
      "File \u001b[1;32mc:\\Users\\benka\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\benka\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\benka\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:246\u001b[0m, in \u001b[0;36mTruncatedSVD.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;241m>\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    243\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be <=\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m n_features(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m         )\n\u001b[1;32m--> 246\u001b[0m     U, Sigma, VT \u001b[38;5;241m=\u001b[39m randomized_svd(\n\u001b[0;32m    247\u001b[0m         X,\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components,\n\u001b[0;32m    249\u001b[0m         n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter,\n\u001b[0;32m    250\u001b[0m         n_oversamples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_oversamples,\n\u001b[0;32m    251\u001b[0m         power_iteration_normalizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpower_iteration_normalizer,\n\u001b[0;32m    252\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[0;32m    253\u001b[0m     )\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_ \u001b[38;5;241m=\u001b[39m VT\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# As a result of the SVD approximation error on X ~ U @ Sigma @ V.T,\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# X @ V is not the same as U @ Sigma\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\benka\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:450\u001b[0m, in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transpose:\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;66;03m# this implementation is a bit faster with smaller shape[1]\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     M \u001b[38;5;241m=\u001b[39m M\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m--> 450\u001b[0m Q \u001b[38;5;241m=\u001b[39m randomized_range_finder(\n\u001b[0;32m    451\u001b[0m     M,\n\u001b[0;32m    452\u001b[0m     size\u001b[38;5;241m=\u001b[39mn_random,\n\u001b[0;32m    453\u001b[0m     n_iter\u001b[38;5;241m=\u001b[39mn_iter,\n\u001b[0;32m    454\u001b[0m     power_iteration_normalizer\u001b[38;5;241m=\u001b[39mpower_iteration_normalizer,\n\u001b[0;32m    455\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[0;32m    456\u001b[0m )\n\u001b[0;32m    458\u001b[0m \u001b[38;5;66;03m# project M to the (k + p) dimensional space using the basis vectors\u001b[39;00m\n\u001b[0;32m    459\u001b[0m B \u001b[38;5;241m=\u001b[39m safe_sparse_dot(Q\u001b[38;5;241m.\u001b[39mT, M)\n",
      "File \u001b[1;32mc:\\Users\\benka\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:286\u001b[0m, in \u001b[0;36mrandomized_range_finder\u001b[1;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[0;32m    282\u001b[0m         Q, _ \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mqr(safe_sparse_dot(A\u001b[38;5;241m.\u001b[39mT, Q), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meconomic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# Sample the range of A using by linear projection of Q\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;66;03m# Extract an orthonormal basis\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m Q, _ \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mqr(safe_sparse_dot(A, Q), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meconomic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Q\n",
      "File \u001b[1;32mc:\\Users\\benka\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\_decomp_qr.py:142\u001b[0m, in \u001b[0;36mqr\u001b[1;34m(a, overwrite_a, lwork, mode, pivoting, check_finite)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     geqrf, \u001b[38;5;241m=\u001b[39m get_lapack_funcs((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeqrf\u001b[39m\u001b[38;5;124m'\u001b[39m,), (a1,))\n\u001b[1;32m--> 142\u001b[0m     qr, tau \u001b[38;5;241m=\u001b[39m safecall(geqrf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeqrf\u001b[39m\u001b[38;5;124m\"\u001b[39m, a1, lwork\u001b[38;5;241m=\u001b[39mlwork,\n\u001b[0;32m    143\u001b[0m                        overwrite_a\u001b[38;5;241m=\u001b[39moverwrite_a)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meconomic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m M \u001b[38;5;241m<\u001b[39m N:\n\u001b[0;32m    146\u001b[0m     R \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mtriu(qr)\n",
      "File \u001b[1;32mc:\\Users\\benka\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\_decomp_qr.py:17\u001b[0m, in \u001b[0;36msafecall\u001b[1;34m(f, name, *args, **kwargs)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lwork \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     16\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlwork\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 17\u001b[0m     ret \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     18\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlwork\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ret[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreal\u001b[38;5;241m.\u001b[39mastype(numpy\u001b[38;5;241m.\u001b[39mint_)\n\u001b[0;32m     19\u001b[0m ret \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reduced_tfidf_matrix = svd.fit_transform(TFV.tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
