{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# import stuff for creating class\n",
    "\n",
    "from abc import ABC, abstractmethod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "class TfIdfBase(ABC):\n",
    "    def __init__(self, corpus, docids, save_folder):\n",
    "        self.corpus = corpus\n",
    "        self.docids = docids\n",
    "        self.save_folder = save_folder\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_idf(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_tf(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def preprocess(self):\n",
    "        pass\n",
    "\n",
    "    def compute_tf_idf(self, do_normalize=True):\n",
    "        self.tf_idf_matrix = self.tf_matrix * self.idf_matrix\n",
    "        # row-wise L2 norm\n",
    "        if do_normalize:\n",
    "            self.tf_idf_matrix = normalize(self.tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stopwordsiso as stopwords\n",
    "import re\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "import os\n",
    "import collections\n",
    "\n",
    "class TfIdfItalian(TfIdfBase):\n",
    "    def __init__(self, corpus, docids, save_folder, overwrite=False):\n",
    "        super().__init__(corpus, docids, save_folder)\n",
    "        self.stopwords = stopwords.stopwords(\"it\")\n",
    "        self.overwrite = overwrite\n",
    "\n",
    "        os.makedirs(f'{self.save_folder}/it', exist_ok=True)\n",
    "\n",
    "    def preprocess(self):\n",
    "\n",
    "        if not self.overwrite and os.path.exists(f'{self.save_folder}/it/word_list.pkl'):\n",
    "            logger.info(\"Loading unique words and word list\")\n",
    "            with open(f'{self.save_folder}/it/word_list.pkl', 'rb') as f:\n",
    "                self.unique_words, self.word_list, self.word_to_index = pickle.load(f)\n",
    "\n",
    "            return  \n",
    "\n",
    "        logger.info(\"Preprocessing corpus\")\n",
    "        start = time.perf_counter()\n",
    "        # self.corpus = [re.sub(r'[^\\w\\s]', doc.lower()) for doc in tqdm(self.corpus)]\n",
    "        # self.corpus = [[word for word in doc.split() if word not in self.stopwords] for doc in tqdm(self.corpus)]\n",
    "\n",
    "        # combine the above 2 into 1\n",
    "        self.word_list = [[word for word in re.sub(r'[^\\w\\s]', '', doc.lower()).split()\\\n",
    "                        if word not in self.stopwords] for doc in tqdm(self.corpus)]\n",
    "\n",
    "\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        self.unique_words = set()\n",
    "        for doc in tqdm(self.word_list):\n",
    "            self.unique_words.update(doc)\n",
    "\n",
    "        self.unique_words = sorted(list(self.unique_words))\n",
    "        self.word_to_index = {word: i for i, word in enumerate(self.unique_words)}\n",
    "\n",
    "        logger.info(f\"Preprocessing took {end-start:.2f} seconds\")\n",
    "        logger.info(f\"Number of unique words: {len(self.unique_words)}\")\n",
    "\n",
    "        logger.info(f\"Saving unique words and word list to {self.save_folder}/it\")\n",
    "        with open(f'{self.save_folder}/it/word_list.pkl', 'wb') as f:\n",
    "            pickle.dump((self.unique_words, self.word_list, self.word_to_index), f)\n",
    "\n",
    "    def compute_tf(self):\n",
    "        # computing term frequency\n",
    "\n",
    "        # if not self.overwrite and os.path.exists(f'{self.save_folder}/en/tf_matrix.npy'):\n",
    "        #     logger.info(\"Loading tf matrix\")\n",
    "        #     self.tf_matrix = np.load(f'{self.save_folder}/en/tf_matrix.npy')\n",
    "        #     return self.tf_matrix\n",
    "\n",
    "        # logger.info(\"Computing term frequency\")\n",
    "        # start = time.perf_counter()\n",
    "\n",
    "        # self.tf_matrix = scipy.sparse.csr_matrix(np.zeros((len(self.docids), len(self.unique_words)), dtype=np.int32))\n",
    "\n",
    "        # for i, doc in enumerate(tqdm(self.word_list)):\n",
    "        #     for word in doc:\n",
    "        #         self.tf_matrix[i, self.word_to_index[word]] += 1\n",
    "\n",
    "        # end = time.perf_counter()\n",
    "        # logger.info(f\"Computing term frequency took {end-start:.2f} seconds\")\n",
    "\n",
    "        # logger.info(f\"Saving tf matrix to {self.save_folder}/en\")\n",
    "        # np.save(f'{self.save_folder}/en/tf_matrix.npy', self.tf_matrix)\n",
    "        # return self.tf_matrix\n",
    "        self._compute_sparse_tf()\n",
    "    \n",
    "\n",
    "    def _compute_sparse_tf(self):\n",
    "        if not self.overwrite and os.path.exists(f'{self.save_folder}/it/sparse_tf_matrix.npy'):\n",
    "            logger.info(\"Loading tf matrix\")\n",
    "            self.tf_matrix = np.load(f'{self.save_folder}/it/sparse_tf_matrix.npy', allow_pickle=True)\n",
    "            return self.tf_matrix\n",
    "        start = time.perf_counter()\n",
    "        row, col, data = [], [], []\n",
    "        for i, doc in enumerate(tqdm(self.word_list)):\n",
    "            word_count = collections.defaultdict(int)\n",
    "            for word in doc:\n",
    "                word_count[word] += 1\n",
    "\n",
    "\n",
    "            for word, count in word_count.items():\n",
    "                row.append(i)\n",
    "                col.append(self.word_to_index[word])\n",
    "                data.append(count)\n",
    "\n",
    "        self.tf_matrix = scipy.sparse.csr_matrix((data, (row, col)), shape=(len(self.docids), len(self.unique_words)), dtype=np.int32)\n",
    "        end = time.perf_counter()\n",
    "        logger.info(f\"Computing term frequency took {end-start:.2f} seconds\")\n",
    "\n",
    "        logger.info(f\"Saving tf matrix to {self.save_folder}/it\")\n",
    "        np.save(f'{self.save_folder}/it/sparse_tf_matrix.npy', self.tf_matrix)\n",
    "        # return self.tf_matrix\n",
    "\n",
    "\n",
    "    def _compute_sparse_idf(self):\n",
    "        if not self.overwrite and os.path.exists(f'{self.save_folder}/it/sparse_idf_matrix.npy'):\n",
    "            logger.info(\"Loading idf matrix\")\n",
    "            self.idf_matrix = np.load(f'{self.save_folder}/it/sparse_idf_matrix.npy', allow_pickle=True)\n",
    "            return self.idf_matrix\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        self.idf_matrix = (self.tf_matrix > 0).sum(axis=0)\n",
    "        self.idf_matrix = np.squeeze(np.array(self.idf_matrix))\n",
    "        self.idf_matrix = np.log((1 + len(self.docids)) / (1 + self.idf_matrix)) + 1\n",
    "        print(self.idf_matrix)\n",
    "        self.idf_matrix = scipy.sparse.diags(self.idf_matrix.astype(np.float32))\n",
    "        end = time.perf_counter()\n",
    "        logger.info(f\"Computing inverse document frequency took {end-start:.2f} seconds\")\n",
    "\n",
    "        logger.info(f\"Saving idf matrix to {self.save_folder}/it\")\n",
    "        np.save(f'{self.save_folder}/it/sparse_idf_matrix.npy', self.idf_matrix)\n",
    "        # return self.idf_matrix\n",
    "    \n",
    "    def compute_idf(self):\n",
    "        # return self._compute_sparse_idf()\n",
    "        self._compute_sparse_idf()\n",
    "\n",
    "\n",
    "    def _get_sparse_query_tfidf(self, queries, use_idf, do_normalize):\n",
    "        query_word_list = [[word for word in re.sub(r'[^\\w\\s]', '', query.lower()).split()\\\n",
    "                        if word not in self.stopwords] for query in queries]\n",
    "\n",
    "        # query_vectors = np.zeros((len(queries), len(self.unique_words)))\n",
    "        # for i in range(len(queries)):\n",
    "        #     for word in query_word_list[i]:\n",
    "        #         if word in self.word_to_index:\n",
    "        #             query_vectors[i][self.word_to_index[word]] = \n",
    "\n",
    "        row, col, data = [], [], []\n",
    "\n",
    "        for i, doc in enumerate(query_word_list):\n",
    "            word_count = collections.defaultdict(int)\n",
    "            for word in doc:\n",
    "                if word in self.word_to_index:\n",
    "                    word_count[word] += 1\n",
    "\n",
    "            for word, count in word_count.items():\n",
    "                row.append(i)\n",
    "                col.append(self.word_to_index[word])\n",
    "                data.append(count)\n",
    "\n",
    "        query_tf_idf_matrix = scipy.sparse.csr_matrix((data, (row, col)), shape=(len(queries), len(self.unique_words)), dtype=np.int32)\n",
    "\n",
    "        if use_idf:\n",
    "            query_tf_idf_matrix = np.array(query_tf_idf_matrix)\n",
    "            query_tf_idf_matrix = query_tf_idf_matrix * self.idf_matrix\n",
    "        \n",
    "        if do_normalize:\n",
    "            query_tf_idf_matrix = normalize(query_tf_idf_matrix)\n",
    "\n",
    "        return query_tf_idf_matrix\n",
    "\n",
    "\n",
    "\n",
    "    def process_queries(self, queries, topk=5, use_idf=False, do_normalize=True):\n",
    "\n",
    "        assert len(queries) == 1\n",
    "        query_tf_idf_matrix = self._get_sparse_query_tfidf(queries, use_idf, do_normalize)\n",
    "\n",
    "        cosine_similarities = self.tf_idf_matrix.dot(query_tf_idf_matrix.T).toarray().flatten()\n",
    "        top_k_indices = np.argsort(cosine_similarities)[::-1][:topk]  # Get top 10 indices by score\n",
    "\n",
    "        return [self.docids[i] for i in top_k_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english_corpus = [\n",
    "#     \"this is the first document psosp\",\n",
    "#     \"this document is the second document ssdfa\",\n",
    "#     \"and this is the third one\",\n",
    "#     \"is this the first document\"\n",
    "# ]\n",
    "# english_docids = list(range(len(english_corpus)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('corpus.json') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_corpus = [doc['text'] for doc in corpus if doc['lang'] == 'it']\n",
    "italian_docids = [doc['docid'] for doc in corpus if doc['lang'] == 'it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFV = TfIdfItalian(italian_corpus, italian_docids, 'tfidf', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-18 16:07:37.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpreprocess\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mLoading unique words and word list\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "TFV.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-18 16:07:40.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_compute_sparse_tf\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mLoading tf matrix\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "TFV.compute_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-18 16:07:41.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_compute_sparse_idf\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mLoading idf matrix\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "TFV.compute_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFV.compute_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dev_set = pd.read_csv('./dev.csv')\n",
    "italian_dev_set = dev_set[dev_set['lang'] == 'it']\n",
    "italian_dev_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_dev_set_queries = italian_dev_set['query'].tolist()\n",
    "italian_dev_set_positive_docs = italian_dev_set['positive_docs'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_10(positive_docs, top_10_ids):\n",
    "    recall = []\n",
    "    for positive_doc, top_10_id in zip(positive_docs, top_10_ids):\n",
    "        recall.append(positive_doc in top_10_id)\n",
    "\n",
    "    print(np.mean(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef59a325910b45b49a6944726827d1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_10_ids = []\n",
    "for dev_query in tqdm(italian_dev_set_queries):\n",
    "    top_10_ids.append(TFV.process_queries([dev_query], 10, True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63\n"
     ]
    }
   ],
   "source": [
    "recall_at_10(italian_dev_set_positive_docs, top_10_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a0c227bca143bcadb9df2d491bfb94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_10_ids = []\n",
    "for dev_query in tqdm(italian_dev_set_queries):\n",
    "    top_10_ids.append(TFV.process_queries([dev_query], 10, False, True))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15b992d29cd4b9ca4163dd937d3508b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_10_ids = []\n",
    "for dev_query in tqdm(italian_dev_set_queries):\n",
    "    top_10_ids.append(TFV.process_queries([dev_query], 10, True, False))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63\n"
     ]
    }
   ],
   "source": [
    "recall_at_10(italian_dev_set_positive_docs, top_10_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd trial\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_tfidf_matrix = svd.fit_transform(TFV.tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
