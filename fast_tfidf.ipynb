{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import numba\n",
    "import pandas as pd\n",
    "import stopwordsiso\n",
    "import Stemmer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordToBase:\n",
    "    \"\"\"\n",
    "    Class to convert words to their base form\n",
    "    \"\"\"\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "\n",
    "    def get_stemmer(self, single=False):\n",
    "        try:\n",
    "            if single:\n",
    "                return Stemmer.Stemmer(self.lang).stemWord\n",
    "            return Stemmer.Stemmer(self.lang).stemWords\n",
    "        except Exception as e:\n",
    "            print(f'Inbuilt Stemmer does not exist for {self.lang}, creating one!')\n",
    "            stemmer = self.create_stemmer(single)\n",
    "        return stemmer\n",
    "    \n",
    "    # to be overwritten -> korean doesn't have stemmer from PyStemmer\n",
    "    def create_stemmer(self, single=False):\n",
    "        if single:\n",
    "            return Stemmer.Stemmer('en').stemWord\n",
    "        return Stemmer.Stemmer('en').stemWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    \"\"\"\n",
    "    Class to preprocess the corpus and queries\n",
    "    \"\"\"\n",
    "    def __init__(self, lang, stopwords=None):\n",
    "        self.lang = lang\n",
    "        self.word_to_base = {}\n",
    "        self.base_to_baseidx = {}\n",
    "        self.word_to_wordidx = {}\n",
    "        self.remove_punct = re.compile(r'(?u)\\b\\w\\w+\\b') # This is what sklearn uses\n",
    "        if stopwords is None:\n",
    "            self.stopwords = set(self.get_stopwords(lang))\n",
    "        else:\n",
    "            self.stopwords = stopwords\n",
    "\n",
    "    def get_stopwords(self, lang):\n",
    "        \"\"\"\n",
    "        Stopwords for the corresponding language\n",
    "        \"\"\"\n",
    "        return stopwordsiso.stopwords(lang)\n",
    "        \n",
    "\n",
    "    def preprocess_corpus(self, corpus):\n",
    "        \"\"\"\n",
    "        Given a corpus, which is a list of documents, we do the following for each document:\n",
    "            - Convert document to lowercase\n",
    "            - For all words in the document, we remove punctuations\n",
    "            - If the word is a stopword, we discard it\n",
    "            - Each word is converted to its base form using the stemmer\n",
    "            - We create several mappings:\n",
    "                • self.word_to_base: word -> base_word\n",
    "                • self.base_to_baseidx: base_word -> base_idx (serves as the vocabulary)\n",
    "                • self.word_to_wordidx: word -> base_idx\n",
    "        We then return a list of lists, where each list is a document, and each element in the list is the base_idx of the word\n",
    "        \"\"\"\n",
    "        stemmer = WordToBase(self.lang).get_stemmer(single=True)\n",
    "        corpus_token_indices = []\n",
    "        for doc in tqdm(corpus):\n",
    "            document_token_indices = []\n",
    "            doc = doc.lower()\n",
    "            words = list(self.remove_punct.findall(doc))\n",
    "\n",
    "            for word in words:\n",
    "                # if we re-encounter the word, we don't need to recompute the base word\n",
    "                if word in self.word_to_wordidx:\n",
    "                    document_token_indices.append(self.word_to_wordidx[word])\n",
    "                    continue\n",
    "                # if we encounter a stopword, we discard it\n",
    "                # Note, this if condition is 2nd as it improves performance\n",
    "                if word in self.stopwords:\n",
    "                    continue\n",
    "                \n",
    "                # if we have already computed the base word, we use it\n",
    "                if word in self.word_to_base:\n",
    "                    base_word = self.word_to_base[word]\n",
    "                # otherwise, we compute the base word\n",
    "                else:\n",
    "                    base_word = stemmer(word)\n",
    "                    self.word_to_base[word] = base_word\n",
    "                # if we have already computed the base_idx, we use it\n",
    "                if base_word in self.base_to_baseidx:\n",
    "                    base_idx = self.base_to_baseidx[base_word]\n",
    "                    self.word_to_wordidx[word] = base_idx\n",
    "                    document_token_indices.append(base_idx)\n",
    "                # else we compute the base_idx and update the mappings\n",
    "                else:\n",
    "                    base_idx = len(self.base_to_baseidx)\n",
    "                    self.base_to_baseidx[base_word] = base_idx\n",
    "                    self.word_to_wordidx[word] = base_idx\n",
    "                    document_token_indices.append(base_idx)\n",
    "            corpus_token_indices.append(document_token_indices)\n",
    "\n",
    "        return corpus_token_indices, self.base_to_baseidx\n",
    "    \n",
    "    def preprocess_queries(self, queries):\n",
    "        \"\"\" \n",
    "        Given a list of queries, we do the following for each query:\n",
    "            - Convert query to lowercase\n",
    "            - For all words in the query, we remove punctuations\n",
    "            - If the word is a stopword, we discard it\n",
    "            - We first form a collection of all the words in the queries\n",
    "            - We then compute the base form of each word\n",
    "            - Then we update all mappings, so that we can convert the query words to base_idx\n",
    "        \"\"\"\n",
    "        query_token_ids = []\n",
    "        word_to_idx = {}\n",
    "        stemmer = WordToBase(self.lang).get_stemmer(single=False)\n",
    "        for query in queries:\n",
    "            query = query.lower() \n",
    "            words = self.remove_punct.findall(query)\n",
    "            query_ids = []\n",
    "            for word in words:\n",
    "                # If we encounter a stopword, we discard it\n",
    "                if word in self.stopwords:\n",
    "                    continue\n",
    "                # If we have not seen the word before, we update the mappings\n",
    "                if word not in word_to_idx:\n",
    "                    word_to_idx[word] = len(word_to_idx)\n",
    "                \n",
    "                # Append the word_idx to the query_ids\n",
    "                word_idx = word_to_idx[word]\n",
    "                query_ids.append(word_idx)\n",
    "            query_token_ids.append(query_ids)\n",
    "\n",
    "        # After the above computation, we have a collection of all query words\n",
    "        # Note that, it is not trivial to use the corpus vocabulary, since queries can have unseen words\n",
    "        # We now compute the base form of each word and update the mappings\n",
    "\n",
    "        # We first get the unique words in the queries\n",
    "        unique_words = list(word_to_idx.keys())\n",
    "        # We then compute the base form of each word\n",
    "        base_words = stemmer(unique_words)\n",
    "        unique_base_words = set(base_words)\n",
    "\n",
    "        # We generate the base_word -> base_idx mapping\n",
    "        unique_base_to_baseidx = {x:i for (i,x) in enumerate(unique_base_words)}\n",
    "\n",
    "        # We create a mapping from word_idx -> base_idx\n",
    "        wordidx_to_baseidx = {word_to_idx[word]:unique_base_to_baseidx[base] for (word, base) in zip(unique_words, base_words)}\n",
    "\n",
    "        # Finally, we convert all to corresponding base words\n",
    "        for i, query_tokens in enumerate(query_token_ids):\n",
    "            query_token_ids[i] = [wordidx_to_baseidx[x] for x in query_tokens]\n",
    "\n",
    "        return query_token_ids, unique_base_to_baseidx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def sift_down(values, indices, startpos, pos):\n",
    "    new_value = values[pos]\n",
    "    new_index = indices[pos]\n",
    "    while pos > startpos:\n",
    "        parentpos = (pos - 1) >> 1\n",
    "        parent_value = values[parentpos]\n",
    "        if new_value < parent_value:\n",
    "            values[pos] = parent_value\n",
    "            indices[pos] = indices[parentpos]\n",
    "            pos = parentpos\n",
    "            continue\n",
    "        break\n",
    "    values[pos] = new_value\n",
    "    indices[pos] = new_index\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def sift_up(values, indices, pos, length):\n",
    "    startpos = pos\n",
    "    new_value = values[pos]\n",
    "    new_index = indices[pos]\n",
    "    childpos = 2 * pos + 1\n",
    "    while childpos < length:\n",
    "        rightpos = childpos + 1\n",
    "        if rightpos < length and values[rightpos] < values[childpos]:\n",
    "            childpos = rightpos\n",
    "        values[pos] = values[childpos]\n",
    "        indices[pos] = indices[childpos]\n",
    "        pos = childpos\n",
    "        childpos = 2 * pos + 1\n",
    "    values[pos] = new_value\n",
    "    indices[pos] = new_index\n",
    "    sift_down(values, indices, startpos, pos)\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def heap_push(values, indices, value, index, length):\n",
    "    values[length] = value\n",
    "    indices[length] = index\n",
    "    sift_down(values, indices, 0, length)\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def heap_pop(values, indices, length):\n",
    "    return_value = values[0]\n",
    "    return_index = indices[0]\n",
    "    last_value = values[length - 1]\n",
    "    last_index = indices[length - 1]\n",
    "    values[0] = last_value\n",
    "    indices[0] = last_index\n",
    "    sift_up(values, indices, 0, length - 1)\n",
    "    return return_value, return_index\n",
    "\n",
    "@numba.njit()\n",
    "def parallel_topk(array, topk):\n",
    "    n = len(array)\n",
    "    if topk > n:\n",
    "        topk = n\n",
    "\n",
    "    values = np.zeros(topk, dtype=array.dtype)  # aka scores\n",
    "    indices = np.zeros(topk, dtype=np.int64)\n",
    "    length = 0\n",
    "\n",
    "    for i, value in enumerate(array):\n",
    "        if length < topk:\n",
    "            heap_push(values, indices, value, i, length)\n",
    "            length += 1\n",
    "        else:\n",
    "            if value > values[0]:\n",
    "                values[0] = value\n",
    "                indices[0] = i\n",
    "                sift_up(values, indices, 0, length)\n",
    "\n",
    "    sorted_indices = np.flip(np.argsort(values))\n",
    "    indices = indices[sorted_indices]\n",
    "    values = values[sorted_indices]\n",
    "\n",
    "    return values, indices\n",
    "\n",
    "@numba.njit\n",
    "def query_token_score(single_query_tokens, score_indptr, indices, data, num_corpus):\n",
    "    start = score_indptr[single_query_tokens]\n",
    "    end = score_indptr[single_query_tokens + 1]\n",
    "    scores = np.zeros(num_corpus, dtype=np.float32)\n",
    "    for j in range(len(single_query_tokens)):\n",
    "        _s, _e = start[j], end[j]\n",
    "        for k in range(_s, _e):\n",
    "            scores[indices[k]] += data[k]\n",
    "\n",
    "    return scores\n",
    "\n",
    "@numba.njit(parallel=True)\n",
    "def all_query_token_score(query_ptrs, query_tokens_ids_flat, topk, score_indptr, indices, data, num_corpus):\n",
    "    topk_scores = np.zeros((len(query_ptrs)-1, topk), dtype=np.float32)\n",
    "    topk_indices = np.zeros((len(query_ptrs)-1, topk), dtype=np.int64)\n",
    "\n",
    "    for i in numba.prange(len(query_ptrs) - 1):\n",
    "        single_query_tokens = query_tokens_ids_flat[query_ptrs[i]:query_ptrs[i+1]]\n",
    "        single_query_score = query_token_score(single_query_tokens, score_indptr, indices, data, num_corpus)\n",
    "        topk_scores_sing, topk_indices_sing = parallel_topk(single_query_score, topk=topk)\n",
    "        topk_scores[i] = topk_scores_sing\n",
    "        topk_indices[i] = topk_indices_sing\n",
    "\n",
    "    return topk_scores, topk_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TFIDF:\n",
    "    def __init__(self, corpus, save_path=None):\n",
    "        if save_path is not None:\n",
    "            self.load(save_path)\n",
    "        else:\n",
    "            self.corpus = corpus \n",
    "            self.num_corpus = len(self.corpus)\n",
    "\n",
    "    def save(self, save_path):\n",
    "        \"\"\"\n",
    "        Save the TF-IDF scores to the disk\n",
    "        \"\"\"\n",
    "        path = f'{save_path}/TFIDF'\n",
    "        np.save(f'{path}/scores_data.npy', self.score_data)\n",
    "        np.save(f'{path}/scores_indices.npy', self.score_indices)\n",
    "        np.save(f'{path}/scores_indptr.npy', self.score_indptr)\n",
    "        with open(f'{path}/token_map.json', 'w') as f:\n",
    "            json.dump(self.token_map, f)\n",
    "        with open(f'params.json', 'w') as f:\n",
    "            json.dump({'num_corpus': self.num_corpus, 'num_unique': self.num_unique}, f)\n",
    "        \n",
    "    def load(self, save_path):\n",
    "        \"\"\" \n",
    "        Load the TF-IDF scores from the disk\n",
    "        \"\"\"\n",
    "        path = f'{save_path}/TFIDF'\n",
    "        self.score_data = np.load(f'{path}/scores_data.npy')\n",
    "        self.score_indices = np.load(f'{path}/scores_indices.npy')\n",
    "        self.score_indptr = np.load(f'{path}/scores_indptr.npy')\n",
    "        with open(f'{path}/token_map.json', 'r') as f:\n",
    "            self.token_map = json.load(f)\n",
    "        with open(f'params.json', 'r') as f:\n",
    "            params = json.load(f)\n",
    "            self.num_corpus = params['num_corpus']\n",
    "            self.num_unique = params\n",
    "\n",
    "    def calculate_scores(self, corpus_tokens, token_map):\n",
    "        \"\"\"\n",
    "        Given the corpus tokens, and the token_map, we calculate the TF-IDF scores\n",
    "        \"\"\"\n",
    "\n",
    "        self.token_map = token_map\n",
    "        unique_tokens = list(token_map.values())\n",
    "        self.num_unique = len(unique_tokens)\n",
    "\n",
    "        # Calculating Document Frequency\n",
    "        set_unique_tokens = set(unique_tokens)\n",
    "        DF = {x: 0 for x in set_unique_tokens}\n",
    "        for document_tokens in corpus_tokens:\n",
    "            tokens_present = set_unique_tokens.intersection(document_tokens)\n",
    "            for token in tokens_present:\n",
    "                DF[token] += 1\n",
    "\n",
    "        self.IDF = self.calculate_idf(DF)\n",
    "\n",
    "        tfidf_scores = np.empty(sum(DF.values()), dtype=np.float32)\n",
    "        doc_indices = np.empty(sum(DF.values()), dtype=np.int64)\n",
    "        word_indices = np.empty(sum(DF.values()), dtype=np.int64)\n",
    "\n",
    "        # calculate tf-idf for each term\n",
    "        ptr = 0\n",
    "        for i, doc_tokens in enumerate(corpus_tokens):\n",
    "            num_tokens = len(doc_tokens)\n",
    "            doc_token_counts = collections.Counter(doc_tokens)\n",
    "            doc_token_indices = np.array(list(doc_token_counts.keys()), dtype=np.int64)\n",
    "            doc_token_counts = np.array(list(doc_token_counts.values()), dtype=np.float32)\n",
    "            \n",
    "            weighted_tf = doc_token_counts / num_tokens\n",
    "            tfidf = weighted_tf * self.IDF[doc_token_indices]\n",
    "            tfidf_scores[ptr:ptr+len(doc_token_indices)] = tfidf\n",
    "            doc_indices[ptr:ptr+len(doc_token_indices)] = i\n",
    "            word_indices[ptr:ptr+len(doc_token_indices)] = doc_token_indices\n",
    "            ptr += len(doc_token_indices)\n",
    "\n",
    "        tfidf_matrix = scipy.sparse.csc_matrix((tfidf_scores, (doc_indices, word_indices)), shape=(self.num_corpus, self.num_unique), dtype=np.float32)\n",
    "        self.tfidf_data = tfidf_matrix.data\n",
    "        self.tfidf_indices = tfidf_matrix.indices\n",
    "        self.tfidf_indptr = tfidf_matrix.indptr\n",
    "\n",
    "    def calculate_idf(self, DF):\n",
    "        \"\"\"\n",
    "        Calculate the inverse document frequency of each token\n",
    "        \"\"\"\n",
    "        IDF = np.zeros(self.num_unique, dtype=np.float32)\n",
    "        for token, _ in DF.items():\n",
    "            IDF[token] = np.log((1 + self.num_corpus) / (1 + DF[token])) + 1\n",
    "        return IDF\n",
    "    \n",
    "    def parallel_search(self, query_tokens, query_token_map, topk=10, num_threads=10):\n",
    "        \"\"\"\n",
    "        Given the query tokens and the query_token_map, we calculate the topk documents for each query\n",
    "        \"\"\"\n",
    "        inverse_query_token_map = {v: k for k, v in query_token_map.items()}\n",
    "        query_tokens = [[inverse_query_token_map[x] for x in query] for query in query_tokens]\n",
    "        query_token_ids = [[self.token_map[x] for x in query if x in self.token_map] for query in query_tokens]\n",
    "        query_tokens_ids_flat = np.concatenate(query_token_ids).astype(np.int64)\n",
    "        query_ptrs = np.cumsum([0] + [len(x) for x in query_token_ids], dtype=np.int64)\n",
    "\n",
    "        numba.set_num_threads(num_threads)\n",
    "        topk_scores, topk_indices = all_query_token_score(query_ptrs, query_tokens_ids_flat, topk, self.tfidf_indptr, self.tfidf_indices, self.tfidf_data, self.num_corpus)\n",
    "        numba.set_num_threads(1)\n",
    "        return topk_indices, topk_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25_V1(TFIDF):\n",
    "    \"\"\"\n",
    "    In BM25_V1, we set the IDF[t] = log((num_corpus - DF[t] + 0.5)/(DF[t] + 0.5) + 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus, k1, b, save_path=None):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        super().__init__(corpus, save_path)\n",
    "\n",
    "    def save(self, save_path):\n",
    "        \"\"\"\n",
    "        Save the TF-IDF scores to the disk\n",
    "        \"\"\"\n",
    "        path = f'{save_path}/BM25_V1'\n",
    "        np.save(f'{path}/scores_data.npy', self.score_data)\n",
    "        np.save(f'{path}/scores_indices.npy', self.score_indices)\n",
    "        np.save(f'{path}/scores_indptr.npy', self.score_indptr)\n",
    "        with open(f'{path}/token_map.json', 'w') as f:\n",
    "            json.dump(self.token_map, f)\n",
    "        with open(f'params.json', 'w') as f:\n",
    "            json.dump({'num_corpus': self.num_corpus, 'num_unique': self.num_unique, 'k1': self.k1, 'b': self.b}, f)\n",
    "        \n",
    "    def load(self, save_path):\n",
    "        \"\"\" \n",
    "        Load the TF-IDF scores from the disk\n",
    "        \"\"\"\n",
    "        path = f'{save_path}/BM25_V1'\n",
    "        self.score_data = np.load(f'{path}/scores_data.npy')\n",
    "        self.score_indices = np.load(f'{path}/scores_indices.npy')\n",
    "        self.score_indptr = np.load(f'{path}/scores_indptr.npy')\n",
    "        with open(f'{path}/token_map.json', 'r') as f:\n",
    "            self.token_map = json.load(f)\n",
    "        with open(f'params.json', 'r') as f:\n",
    "            params = json.load(f)\n",
    "            self.num_corpus = params['num_corpus']\n",
    "            self.num_unique = params['num_unique']\n",
    "            self.k1 = params['k1']\n",
    "            self.b = params['b']\n",
    "\n",
    "    def calculate_scores(self, corpus_tokens, token_map):\n",
    "        \"\"\"\n",
    "        Given the corpus tokens, and the token_map, we calculate the BM25 scores\n",
    "        \"\"\"\n",
    "\n",
    "        self.token_map = token_map\n",
    "        unique_tokens = list(token_map.values())\n",
    "        self.num_unique = len(unique_tokens)\n",
    "        L_avg = sum([len(x) for x in corpus_tokens]) / len(corpus_tokens)\n",
    "\n",
    "        # Calculating Document Frequency\n",
    "        set_unique_tokens = set(unique_tokens)\n",
    "        DF = {x: 0 for x in set_unique_tokens}\n",
    "        for document_tokens in corpus_tokens:\n",
    "            tokens_present = set_unique_tokens.intersection(document_tokens)\n",
    "            for token in tokens_present:\n",
    "                DF[token] += 1\n",
    "\n",
    "        self.IDF = self.calculate_idf(DF)\n",
    "\n",
    "        tfidf_scores = np.empty(sum(DF.values()), dtype=np.float32)\n",
    "        doc_indices = np.empty(sum(DF.values()), dtype=np.int64)\n",
    "        word_indices = np.empty(sum(DF.values()), dtype=np.int64)\n",
    "\n",
    "        # calculate tf-idf for each term\n",
    "        ptr = 0\n",
    "        for i, doc_tokens in enumerate(corpus_tokens):\n",
    "            num_tokens = len(doc_tokens)\n",
    "            doc_token_counts = collections.Counter(doc_tokens)\n",
    "            doc_token_indices = np.array(list(doc_token_counts.keys()), dtype=np.int64)\n",
    "            doc_token_counts = np.array(list(doc_token_counts.values()), dtype=np.float32)\n",
    "            \n",
    "            weighted_tf = doc_token_counts / (doc_token_counts + self.k1 * (1 - self.b*(1 - num_tokens/L_avg)))\n",
    "            tfidf = weighted_tf * self.IDF[doc_token_indices]\n",
    "            tfidf_scores[ptr:ptr+len(doc_token_indices)] = tfidf\n",
    "            doc_indices[ptr:ptr+len(doc_token_indices)] = i\n",
    "            word_indices[ptr:ptr+len(doc_token_indices)] = doc_token_indices\n",
    "            ptr += len(doc_token_indices)\n",
    "\n",
    "        tfidf_matrix = scipy.sparse.csc_matrix((tfidf_scores, (doc_indices, word_indices)), shape=(self.num_corpus, self.num_unique), dtype=np.float32)\n",
    "        self.tfidf_data = tfidf_matrix.data\n",
    "        self.tfidf_indices = tfidf_matrix.indices\n",
    "        self.tfidf_indptr = tfidf_matrix.indptr\n",
    "\n",
    "    def calculate_idf(self, DF):\n",
    "        \"\"\"\n",
    "        Calculate the inverse document frequency of each token\n",
    "        \"\"\"\n",
    "        IDF = np.zeros(self.num_unique, dtype=np.float32)\n",
    "        for token, _ in DF.items():\n",
    "            IDF[token] = np.log((self.num_corpus - DF[token] + 0.5)/(DF[token] + 0.5) + 1)\n",
    "        return IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25_V2(BM25_V1):\n",
    "    \"\"\"\n",
    "    In BM25_V2, we set the IDF[t] = log((1 + num_corpus)/(1 + DocFreq[t])) + 1\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus, k1, b, save_path=None):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        super().__init__(corpus, k1, b, save_path)\n",
    "\n",
    "    def save(self, save_path):\n",
    "        \"\"\"\n",
    "        Save the TF-IDF scores to the disk\n",
    "        \"\"\"\n",
    "        path = f'{save_path}/BM25_V2'\n",
    "        np.save(f'{path}/scores_data.npy', self.score_data)\n",
    "        np.save(f'{path}/scores_indices.npy', self.score_indices)\n",
    "        np.save(f'{path}/scores_indptr.npy', self.score_indptr)\n",
    "        with open(f'{path}/token_map.json', 'w') as f:\n",
    "            json.dump(self.token_map, f)\n",
    "        with open(f'params.json', 'w') as f:\n",
    "            json.dump({'num_corpus': self.num_corpus, 'num_unique': self.num_unique, 'k1': self.k1, 'b': self.b}, f)\n",
    "        \n",
    "    def load(self, save_path):\n",
    "        \"\"\" \n",
    "        Load the TF-IDF scores from the disk\n",
    "        \"\"\"\n",
    "        path = f'{save_path}/BM25_V2'\n",
    "        self.score_data = np.load(f'{path}/scores_data.npy')\n",
    "        self.score_indices = np.load(f'{path}/scores_indices.npy')\n",
    "        self.score_indptr = np.load(f'{path}/scores_indptr.npy')\n",
    "        with open(f'{path}/token_map.json', 'r') as f:\n",
    "            self.token_map = json.load(f)\n",
    "        with open(f'params.json', 'r') as f:\n",
    "            params = json.load(f)\n",
    "            self.num_corpus = params['num_corpus']\n",
    "            self.num_unique = params['num_unique']\n",
    "            self.k1 = params['k1']\n",
    "            self.b = params['b']\n",
    "\n",
    "    def calculate_idf(self, DF):\n",
    "        \"\"\"\n",
    "        Calculate the inverse document frequency of each token\n",
    "        \"\"\"\n",
    "        IDF = np.zeros(self.num_unique, dtype=np.float32)\n",
    "        for token, _ in DF.items():\n",
    "            IDF[token] = np.log((1 + self.num_corpus) / (1 + DF[token])) + 1\n",
    "        return IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "corpus = json.load(open('../dis-project-1-document-retrieval/corpus.json/corpus.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_FRENCH = (\n",
    "    \"ai\",\n",
    "    \"aie\",\n",
    "    \"aient\",\n",
    "    \"aies\",\n",
    "    \"ait\",\n",
    "    \"as\",\n",
    "    \"au\",\n",
    "    \"aura\",\n",
    "    \"aurai\",\n",
    "    \"auraient\",\n",
    "    \"aurais\",\n",
    "    \"aurait\",\n",
    "    \"auras\",\n",
    "    \"aurez\",\n",
    "    \"auriez\",\n",
    "    \"aurions\",\n",
    "    \"aurons\",\n",
    "    \"auront\",\n",
    "    \"aux\",\n",
    "    \"avaient\",\n",
    "    \"avais\",\n",
    "    \"avait\",\n",
    "    \"avec\",\n",
    "    \"avez\",\n",
    "    \"aviez\",\n",
    "    \"avions\",\n",
    "    \"avons\",\n",
    "    \"ayant\",\n",
    "    \"ayante\",\n",
    "    \"ayantes\",\n",
    "    \"ayants\",\n",
    "    \"ayez\",\n",
    "    \"ayons\",\n",
    "    \"c\",\n",
    "    \"ce\",\n",
    "    \"ces\",\n",
    "    \"d\",\n",
    "    \"dans\",\n",
    "    \"de\",\n",
    "    \"des\",\n",
    "    \"du\",\n",
    "    \"elle\",\n",
    "    \"en\",\n",
    "    \"es\",\n",
    "    \"est\",\n",
    "    \"et\",\n",
    "    \"eu\",\n",
    "    \"eue\",\n",
    "    \"eues\",\n",
    "    \"eurent\",\n",
    "    \"eus\",\n",
    "    \"eusse\",\n",
    "    \"eussent\",\n",
    "    \"eusses\",\n",
    "    \"eussiez\",\n",
    "    \"eussions\",\n",
    "    \"eut\",\n",
    "    \"eux\",\n",
    "    \"eûmes\",\n",
    "    \"eût\",\n",
    "    \"eûtes\",\n",
    "    \"furent\",\n",
    "    \"fus\",\n",
    "    \"fusse\",\n",
    "    \"fussent\",\n",
    "    \"fusses\",\n",
    "    \"fussiez\",\n",
    "    \"fussions\",\n",
    "    \"fut\",\n",
    "    \"fûmes\",\n",
    "    \"fût\",\n",
    "    \"fûtes\",\n",
    "    \"il\",\n",
    "    \"ils\",\n",
    "    \"j\",\n",
    "    \"je\",\n",
    "    \"l\",\n",
    "    \"la\",\n",
    "    \"le\",\n",
    "    \"les\",\n",
    "    \"leur\",\n",
    "    \"lui\",\n",
    "    \"m\",\n",
    "    \"ma\",\n",
    "    \"mais\",\n",
    "    \"me\",\n",
    "    \"mes\",\n",
    "    \"moi\",\n",
    "    \"mon\",\n",
    "    \"même\",\n",
    "    \"n\",\n",
    "    \"ne\",\n",
    "    \"nos\",\n",
    "    \"notre\",\n",
    "    \"nous\",\n",
    "    \"on\",\n",
    "    \"ont\",\n",
    "    \"ou\",\n",
    "    \"par\",\n",
    "    \"pas\",\n",
    "    \"pour\",\n",
    "    \"qu\",\n",
    "    \"que\",\n",
    "    \"qui\",\n",
    "    \"s\",\n",
    "    \"sa\",\n",
    "    \"se\",\n",
    "    \"sera\",\n",
    "    \"serai\",\n",
    "    \"seraient\",\n",
    "    \"serais\",\n",
    "    \"serait\",\n",
    "    \"seras\",\n",
    "    \"serez\",\n",
    "    \"seriez\",\n",
    "    \"serions\",\n",
    "    \"serons\",\n",
    "    \"seront\",\n",
    "    \"ses\",\n",
    "    \"soient\",\n",
    "    \"sois\",\n",
    "    \"soit\",\n",
    "    \"sommes\",\n",
    "    \"son\",\n",
    "    \"sont\",\n",
    "    \"soyez\",\n",
    "    \"soyons\",\n",
    "    \"suis\",\n",
    "    \"sur\",\n",
    "    \"t\",\n",
    "    \"ta\",\n",
    "    \"te\",\n",
    "    \"tes\",\n",
    "    \"toi\",\n",
    "    \"ton\",\n",
    "    \"tu\",\n",
    "    \"un\",\n",
    "    \"une\",\n",
    "    \"vos\",\n",
    "    \"votre\",\n",
    "    \"vous\",\n",
    "    \"y\",\n",
    "    \"à\",\n",
    "    \"étaient\",\n",
    "    \"étais\",\n",
    "    \"était\",\n",
    "    \"étant\",\n",
    "    \"étante\",\n",
    "    \"étantes\",\n",
    "    \"étants\",\n",
    "    \"étiez\",\n",
    "    \"étions\",\n",
    "    \"été\",\n",
    "    \"étée\",\n",
    "    \"étées\",\n",
    "    \"étés\",\n",
    "    \"êtes\",\n",
    ")\n",
    "\n",
    "STOPWORDS_EN = (\n",
    "    \"a\",\n",
    "    \"an\",\n",
    "    \"and\",\n",
    "    \"are\",\n",
    "    \"as\",\n",
    "    \"at\",\n",
    "    \"be\",\n",
    "    \"but\",\n",
    "    \"by\",\n",
    "    \"for\",\n",
    "    \"if\",\n",
    "    \"in\",\n",
    "    \"into\",\n",
    "    \"is\",\n",
    "    \"it\",\n",
    "    \"no\",\n",
    "    \"not\",\n",
    "    \"of\",\n",
    "    \"on\",\n",
    "    \"or\",\n",
    "    \"such\",\n",
    "    \"that\",\n",
    "    \"the\",\n",
    "    \"their\",\n",
    "    \"then\",\n",
    "    \"there\",\n",
    "    \"these\",\n",
    "    \"they\",\n",
    "    \"this\",\n",
    "    \"to\",\n",
    "    \"was\",\n",
    "    \"will\",\n",
    "    \"with\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10676/10676 [00:16<00:00, 634.32it/s]\n"
     ]
    }
   ],
   "source": [
    "french_text = [x['text'] for x in corpus if x['lang'] == 'fr']\n",
    "french_corpus = [x for x in corpus if x['lang'] == 'fr']\n",
    "P = TextProcessor('french', stopwords=STOPWORDS_EN)\n",
    "corpus_tokens, corpus_map = P.preprocess_corpus(french_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv('../dis-project-1-document-retrieval/dev.csv')\n",
    "my_dev_df = dev_df[dev_df['lang'] == 'fr']\n",
    "my_dev_set_queries = my_dev_df['query'].tolist()\n",
    "my_dev_set_positive_docs = my_dev_df['positive_docs'].tolist()\n",
    "query_tokens, query_map = P.preprocess_queries(my_dev_set_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_docid(idx_list_list, french_c_with_id):\n",
    "    res = []\n",
    "    for idx_list in idx_list_list:\n",
    "        res.append(list(map(lambda x: french_c_with_id[x]['docid'], idx_list)))\n",
    "    return res\n",
    "def recall_at_10(positive_docs, top_10_ids):\n",
    "    recall = []\n",
    "    for positive_doc, top_10_id in zip(positive_docs, top_10_ids):\n",
    "        recall.append(positive_doc in top_10_id)\n",
    "\n",
    "    print(np.mean(recall))\n",
    "french_c_with_id = [x for x in corpus if x['lang'] == 'fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.265\n"
     ]
    }
   ],
   "source": [
    "tf_idf = TFIDF(french_text)\n",
    "tf_idf.calculate_scores(corpus_tokens, corpus_map)\n",
    "results, scores = tf_idf.parallel_search(query_tokens, query_map, topk=10, num_threads=1)\n",
    "s = idx_to_docid(results, french_c_with_id)\n",
    "recall_at_10(my_dev_set_positive_docs, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.895\n"
     ]
    }
   ],
   "source": [
    "tf_idf = BM25_V1(french_text, k1=1.5, b=0.75)\n",
    "tf_idf.calculate_scores(corpus_tokens, corpus_map)\n",
    "results, scores = tf_idf.parallel_search(query_tokens, query_map, topk=10, num_threads=1)\n",
    "s = idx_to_docid(results, french_c_with_id)\n",
    "recall_at_10(my_dev_set_positive_docs, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "tf_idf = BM25_V2(french_text, k1=1.5, b=0.75, save_path=None)\n",
    "tf_idf.calculate_scores(corpus_tokens, corpus_map)\n",
    "results, scores = tf_idf.parallel_search(query_tokens, query_map, topk=10, num_threads=1)\n",
    "s = idx_to_docid(results, french_c_with_id)\n",
    "recall_at_10(my_dev_set_positive_docs, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "class A:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        self.mul()\n",
    "    def mul(self):\n",
    "        print(self.x * 2)\n",
    "class B(A):\n",
    "    def __init__(self, x):\n",
    "        super().__init__(x)\n",
    "    def mul(self):\n",
    "        print(self.x * 3)\n",
    "\n",
    "b = B(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs423-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
